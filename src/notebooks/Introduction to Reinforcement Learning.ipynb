{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning\n",
    "\n",
    "This notebook is a walkthrough of applying reinforcement learning on different problems using the keras and tensorflow backend neural network over Q-Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Defining the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions available in the Cartpole problem\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Defining the ANN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 86\n",
      "Trainable params: 86\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: Training the Model using the Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "   10/5000: episode: 1, duration: 0.044s, episode steps: 10, steps per second: 227, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.967, 3.014], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   18/5000: episode: 2, duration: 0.454s, episode steps: 8, steps per second: 18, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.172 [-1.323, 2.220], loss: 0.496810, mean_absolute_error: 0.524521, mean_q: 0.138401\n",
      "   29/5000: episode: 3, duration: 0.016s, episode steps: 11, steps per second: 673, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.150 [-1.711, 2.817], loss: 0.472517, mean_absolute_error: 0.522254, mean_q: 0.188424\n",
      "   39/5000: episode: 4, duration: 0.016s, episode steps: 10, steps per second: 635, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.117 [-1.589, 2.511], loss: 0.433295, mean_absolute_error: 0.501396, mean_q: 0.242052\n",
      "   52/5000: episode: 5, duration: 0.019s, episode steps: 13, steps per second: 687, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.077 [0.000, 1.000], mean observation: 0.125 [-2.110, 3.295], loss: 0.399056, mean_absolute_error: 0.479174, mean_q: 0.291243\n",
      "   61/5000: episode: 6, duration: 0.013s, episode steps: 9, steps per second: 676, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.753, 2.794], loss: 0.350481, mean_absolute_error: 0.441820, mean_q: 0.365662\n",
      "   71/5000: episode: 7, duration: 0.015s, episode steps: 10, steps per second: 675, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [-1.913, 3.101], loss: 0.330227, mean_absolute_error: 0.429066, mean_q: 0.426781\n",
      "   80/5000: episode: 8, duration: 0.013s, episode steps: 9, steps per second: 703, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.763, 2.739], loss: 0.305914, mean_absolute_error: 0.408493, mean_q: 0.490416\n",
      "   89/5000: episode: 9, duration: 0.013s, episode steps: 9, steps per second: 700, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [-1.716, 2.815], loss: 0.273380, mean_absolute_error: 0.380005, mean_q: 0.572893\n",
      "   98/5000: episode: 10, duration: 0.013s, episode steps: 9, steps per second: 693, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.779, 2.894], loss: 0.257918, mean_absolute_error: 0.363714, mean_q: 0.624367\n",
      "  107/5000: episode: 11, duration: 0.013s, episode steps: 9, steps per second: 705, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.787, 2.754], loss: 0.248207, mean_absolute_error: 0.364429, mean_q: 0.697286\n",
      "  117/5000: episode: 12, duration: 0.014s, episode steps: 10, steps per second: 716, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.158 [-1.915, 3.108], loss: 0.255560, mean_absolute_error: 0.371762, mean_q: 0.715412\n",
      "  128/5000: episode: 13, duration: 0.015s, episode steps: 11, steps per second: 717, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.138 [-1.719, 2.755], loss: 0.243511, mean_absolute_error: 0.368208, mean_q: 0.805300\n",
      "  138/5000: episode: 14, duration: 0.014s, episode steps: 10, steps per second: 712, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.137 [-1.550, 2.498], loss: 0.235963, mean_absolute_error: 0.368494, mean_q: 0.866717\n",
      "  147/5000: episode: 15, duration: 0.015s, episode steps: 9, steps per second: 596, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.768, 2.831], loss: 0.259957, mean_absolute_error: 0.399097, mean_q: 0.903846\n",
      "  155/5000: episode: 16, duration: 0.012s, episode steps: 8, steps per second: 664, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.558, 2.523], loss: 0.235998, mean_absolute_error: 0.394486, mean_q: 1.024429\n",
      "  165/5000: episode: 17, duration: 0.015s, episode steps: 10, steps per second: 664, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.126 [-1.937, 3.018], loss: 0.228474, mean_absolute_error: 0.395300, mean_q: 1.065505\n",
      "  175/5000: episode: 18, duration: 0.014s, episode steps: 10, steps per second: 709, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.965, 3.056], loss: 0.226434, mean_absolute_error: 0.402479, mean_q: 1.148300\n",
      "  185/5000: episode: 19, duration: 0.014s, episode steps: 10, steps per second: 703, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.173 [-1.932, 3.121], loss: 0.253825, mean_absolute_error: 0.444784, mean_q: 1.194093\n",
      "  194/5000: episode: 20, duration: 0.013s, episode steps: 9, steps per second: 697, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.756, 2.807], loss: 0.243615, mean_absolute_error: 0.445397, mean_q: 1.272690\n",
      "  202/5000: episode: 21, duration: 0.012s, episode steps: 8, steps per second: 691, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.169 [-1.546, 2.557], loss: 0.264988, mean_absolute_error: 0.473708, mean_q: 1.352695\n",
      "  212/5000: episode: 22, duration: 0.016s, episode steps: 10, steps per second: 634, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.137 [-1.580, 2.564], loss: 0.282428, mean_absolute_error: 0.491198, mean_q: 1.364615\n",
      "  220/5000: episode: 23, duration: 0.015s, episode steps: 8, steps per second: 545, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.157 [-1.531, 2.553], loss: 0.267370, mean_absolute_error: 0.499370, mean_q: 1.455222\n",
      "  230/5000: episode: 24, duration: 0.015s, episode steps: 10, steps per second: 687, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.145 [-1.553, 2.544], loss: 0.261839, mean_absolute_error: 0.498088, mean_q: 1.449518\n",
      "  240/5000: episode: 25, duration: 0.015s, episode steps: 10, steps per second: 685, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.120 [-1.969, 3.033], loss: 0.311278, mean_absolute_error: 0.556947, mean_q: 1.495324\n",
      "  248/5000: episode: 26, duration: 0.012s, episode steps: 8, steps per second: 684, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.137 [-1.355, 2.208], loss: 0.339468, mean_absolute_error: 0.589038, mean_q: 1.580886\n",
      "  258/5000: episode: 27, duration: 0.014s, episode steps: 10, steps per second: 695, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [-1.964, 3.004], loss: 0.342270, mean_absolute_error: 0.600700, mean_q: 1.628694\n",
      "  267/5000: episode: 28, duration: 0.013s, episode steps: 9, steps per second: 694, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.810, 2.873], loss: 0.363076, mean_absolute_error: 0.644984, mean_q: 1.697911\n",
      "  277/5000: episode: 29, duration: 0.016s, episode steps: 10, steps per second: 625, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.175 [-1.910, 3.113], loss: 0.389565, mean_absolute_error: 0.666708, mean_q: 1.788340\n",
      "  286/5000: episode: 30, duration: 0.014s, episode steps: 9, steps per second: 642, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.717, 2.796], loss: 0.319979, mean_absolute_error: 0.657675, mean_q: 1.829442\n",
      "  296/5000: episode: 31, duration: 0.015s, episode steps: 10, steps per second: 665, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [-1.985, 3.032], loss: 0.364707, mean_absolute_error: 0.684075, mean_q: 1.829527\n",
      "  305/5000: episode: 32, duration: 0.013s, episode steps: 9, steps per second: 668, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.807, 2.880], loss: 0.384248, mean_absolute_error: 0.712165, mean_q: 1.911354\n",
      "  314/5000: episode: 33, duration: 0.013s, episode steps: 9, steps per second: 687, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.737, 2.764], loss: 0.429524, mean_absolute_error: 0.749709, mean_q: 2.006031\n",
      "  325/5000: episode: 34, duration: 0.016s, episode steps: 11, steps per second: 691, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.139 [-1.325, 2.327], loss: 0.379860, mean_absolute_error: 0.748089, mean_q: 2.033086\n",
      "  333/5000: episode: 35, duration: 0.012s, episode steps: 8, steps per second: 675, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.590, 2.565], loss: 0.411744, mean_absolute_error: 0.777056, mean_q: 2.105113\n",
      "  343/5000: episode: 36, duration: 0.016s, episode steps: 10, steps per second: 643, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.977, 3.069], loss: 0.530220, mean_absolute_error: 0.859665, mean_q: 2.165666\n",
      "  353/5000: episode: 37, duration: 0.015s, episode steps: 10, steps per second: 669, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.135 [-1.937, 3.009], loss: 0.438662, mean_absolute_error: 0.859361, mean_q: 2.157731\n",
      "  361/5000: episode: 38, duration: 0.012s, episode steps: 8, steps per second: 681, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.547, 2.531], loss: 0.420106, mean_absolute_error: 0.857472, mean_q: 2.254529\n",
      "  371/5000: episode: 39, duration: 0.015s, episode steps: 10, steps per second: 689, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.958, 3.050], loss: 0.506039, mean_absolute_error: 0.894712, mean_q: 2.295970\n",
      "  381/5000: episode: 40, duration: 0.015s, episode steps: 10, steps per second: 653, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.109 [-1.579, 2.508], loss: 0.486248, mean_absolute_error: 0.931762, mean_q: 2.388499\n",
      "  391/5000: episode: 41, duration: 0.015s, episode steps: 10, steps per second: 689, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.153 [-1.567, 2.627], loss: 0.508350, mean_absolute_error: 0.950254, mean_q: 2.393300\n",
      "  402/5000: episode: 42, duration: 0.016s, episode steps: 11, steps per second: 709, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.115 [-1.810, 2.840], loss: 0.600293, mean_absolute_error: 1.003982, mean_q: 2.475768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  410/5000: episode: 43, duration: 0.013s, episode steps: 8, steps per second: 618, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.174 [-1.530, 2.589], loss: 0.583812, mean_absolute_error: 1.042709, mean_q: 2.516412\n",
      "  420/5000: episode: 44, duration: 0.017s, episode steps: 10, steps per second: 605, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [-1.915, 3.070], loss: 0.445071, mean_absolute_error: 0.972192, mean_q: 2.464103\n",
      "  429/5000: episode: 45, duration: 0.015s, episode steps: 9, steps per second: 610, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.167 [-1.721, 2.843], loss: 0.497742, mean_absolute_error: 1.029385, mean_q: 2.709285\n",
      "  438/5000: episode: 46, duration: 0.014s, episode steps: 9, steps per second: 643, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.187 [-1.727, 2.899], loss: 0.480744, mean_absolute_error: 1.008800, mean_q: 2.744985\n",
      "  447/5000: episode: 47, duration: 0.013s, episode steps: 9, steps per second: 686, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.164 [-1.721, 2.789], loss: 0.439130, mean_absolute_error: 1.010192, mean_q: 2.834392\n",
      "  458/5000: episode: 48, duration: 0.017s, episode steps: 11, steps per second: 664, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.105 [-1.795, 2.820], loss: 0.503137, mean_absolute_error: 1.048971, mean_q: 2.943170\n",
      "  468/5000: episode: 49, duration: 0.015s, episode steps: 10, steps per second: 688, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.122 [-1.574, 2.524], loss: 0.581622, mean_absolute_error: 1.109857, mean_q: 2.984233\n",
      "  478/5000: episode: 50, duration: 0.018s, episode steps: 10, steps per second: 544, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.124 [-1.976, 3.042], loss: 0.479278, mean_absolute_error: 1.110518, mean_q: 2.910091\n",
      "  488/5000: episode: 51, duration: 0.015s, episode steps: 10, steps per second: 676, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.125 [-1.533, 2.509], loss: 0.405790, mean_absolute_error: 1.097404, mean_q: 2.960077\n",
      "  498/5000: episode: 52, duration: 0.015s, episode steps: 10, steps per second: 686, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.124 [-1.986, 3.002], loss: 0.420742, mean_absolute_error: 1.129776, mean_q: 3.103003\n",
      "  508/5000: episode: 53, duration: 0.014s, episode steps: 10, steps per second: 699, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.994, 3.014], loss: 0.391749, mean_absolute_error: 1.141314, mean_q: 3.182651\n",
      "  519/5000: episode: 54, duration: 0.016s, episode steps: 11, steps per second: 696, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.120 [-1.183, 1.939], loss: 0.412359, mean_absolute_error: 1.191432, mean_q: 3.216481\n",
      "  529/5000: episode: 55, duration: 0.014s, episode steps: 10, steps per second: 693, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.138 [-1.549, 2.572], loss: 0.476470, mean_absolute_error: 1.266844, mean_q: 3.247619\n",
      "  539/5000: episode: 56, duration: 0.017s, episode steps: 10, steps per second: 573, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.109 [-1.592, 2.380], loss: 0.454331, mean_absolute_error: 1.277157, mean_q: 3.225322\n",
      "  548/5000: episode: 57, duration: 0.014s, episode steps: 9, steps per second: 623, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.775, 2.809], loss: 0.431344, mean_absolute_error: 1.302294, mean_q: 3.231524\n",
      "  558/5000: episode: 58, duration: 0.015s, episode steps: 10, steps per second: 681, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.927, 3.118], loss: 0.431533, mean_absolute_error: 1.321488, mean_q: 3.329016\n",
      "  566/5000: episode: 59, duration: 0.012s, episode steps: 8, steps per second: 684, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [-1.605, 2.571], loss: 0.420895, mean_absolute_error: 1.330837, mean_q: 3.366247\n",
      "  576/5000: episode: 60, duration: 0.014s, episode steps: 10, steps per second: 691, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.997, 3.097], loss: 0.391240, mean_absolute_error: 1.348047, mean_q: 3.445504\n",
      "  585/5000: episode: 61, duration: 0.013s, episode steps: 9, steps per second: 675, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [-1.773, 2.767], loss: 0.256270, mean_absolute_error: 1.303970, mean_q: 3.526340\n",
      "  595/5000: episode: 62, duration: 0.014s, episode steps: 10, steps per second: 695, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.950, 3.076], loss: 0.458058, mean_absolute_error: 1.390624, mean_q: 3.672995\n",
      "  606/5000: episode: 63, duration: 0.016s, episode steps: 11, steps per second: 701, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-2.107, 3.282], loss: 0.438657, mean_absolute_error: 1.425205, mean_q: 3.582143\n",
      "  616/5000: episode: 64, duration: 0.015s, episode steps: 10, steps per second: 680, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.913, 3.061], loss: 0.332403, mean_absolute_error: 1.395736, mean_q: 3.659126\n",
      "  626/5000: episode: 65, duration: 0.015s, episode steps: 10, steps per second: 689, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.119 [-1.968, 3.016], loss: 0.468168, mean_absolute_error: 1.457061, mean_q: 3.647802\n",
      "  636/5000: episode: 66, duration: 0.014s, episode steps: 10, steps per second: 698, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.937, 3.059], loss: 0.358164, mean_absolute_error: 1.456923, mean_q: 3.686675\n",
      "  648/5000: episode: 67, duration: 0.017s, episode steps: 12, steps per second: 704, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.098 [-1.569, 2.454], loss: 0.376824, mean_absolute_error: 1.460168, mean_q: 3.823017\n",
      "  660/5000: episode: 68, duration: 0.017s, episode steps: 12, steps per second: 704, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.117 [-1.547, 2.577], loss: 0.371680, mean_absolute_error: 1.504338, mean_q: 3.927818\n",
      "  670/5000: episode: 69, duration: 0.015s, episode steps: 10, steps per second: 681, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.942, 3.066], loss: 0.301534, mean_absolute_error: 1.521764, mean_q: 3.990205\n",
      "  680/5000: episode: 70, duration: 0.017s, episode steps: 10, steps per second: 593, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [-1.940, 3.084], loss: 0.365229, mean_absolute_error: 1.536875, mean_q: 4.097212\n",
      "  689/5000: episode: 71, duration: 0.014s, episode steps: 9, steps per second: 649, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.145 [-1.419, 2.295], loss: 0.333961, mean_absolute_error: 1.558910, mean_q: 4.099435\n",
      "  697/5000: episode: 72, duration: 0.012s, episode steps: 8, steps per second: 678, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.157 [-1.537, 2.570], loss: 0.388283, mean_absolute_error: 1.614850, mean_q: 4.032965\n",
      "  707/5000: episode: 73, duration: 0.014s, episode steps: 10, steps per second: 691, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.117 [-1.611, 2.534], loss: 0.320697, mean_absolute_error: 1.613838, mean_q: 4.134593\n",
      "  715/5000: episode: 74, duration: 0.012s, episode steps: 8, steps per second: 667, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.163 [-1.532, 2.551], loss: 0.316914, mean_absolute_error: 1.630818, mean_q: 4.267723\n",
      "  725/5000: episode: 75, duration: 0.014s, episode steps: 10, steps per second: 701, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.128 [-1.546, 2.524], loss: 0.324118, mean_absolute_error: 1.664639, mean_q: 4.225266\n",
      "  734/5000: episode: 76, duration: 0.013s, episode steps: 9, steps per second: 684, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.167 [-1.718, 2.807], loss: 0.334655, mean_absolute_error: 1.687198, mean_q: 4.246774\n",
      "  749/5000: episode: 77, duration: 0.021s, episode steps: 15, steps per second: 710, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.133 [0.000, 1.000], mean observation: 0.067 [-2.118, 3.150], loss: 0.289165, mean_absolute_error: 1.706512, mean_q: 4.313150\n",
      "  757/5000: episode: 78, duration: 0.013s, episode steps: 8, steps per second: 616, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [-1.525, 2.518], loss: 0.350675, mean_absolute_error: 1.765849, mean_q: 4.437519\n",
      "  766/5000: episode: 79, duration: 0.013s, episode steps: 9, steps per second: 684, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.726, 2.792], loss: 0.263946, mean_absolute_error: 1.754955, mean_q: 4.472143\n",
      "  778/5000: episode: 80, duration: 0.017s, episode steps: 12, steps per second: 699, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.121 [-1.912, 3.078], loss: 0.290596, mean_absolute_error: 1.797099, mean_q: 4.516426\n",
      "  787/5000: episode: 81, duration: 0.013s, episode steps: 9, steps per second: 676, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.748, 2.806], loss: 0.374334, mean_absolute_error: 1.859033, mean_q: 4.392572\n",
      "  797/5000: episode: 82, duration: 0.015s, episode steps: 10, steps per second: 682, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.912, 3.055], loss: 0.269506, mean_absolute_error: 1.855228, mean_q: 4.521144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  808/5000: episode: 83, duration: 0.018s, episode steps: 11, steps per second: 618, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.122 [-1.784, 2.810], loss: 0.261569, mean_absolute_error: 1.908658, mean_q: 4.688330\n",
      "  817/5000: episode: 84, duration: 0.014s, episode steps: 9, steps per second: 651, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.716, 2.761], loss: 0.290721, mean_absolute_error: 1.954617, mean_q: 4.740277\n",
      "  826/5000: episode: 85, duration: 0.013s, episode steps: 9, steps per second: 677, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.772, 2.814], loss: 0.236059, mean_absolute_error: 1.965842, mean_q: 4.769562\n",
      "  835/5000: episode: 86, duration: 0.013s, episode steps: 9, steps per second: 677, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.750, 2.767], loss: 0.246997, mean_absolute_error: 1.989232, mean_q: 4.720316\n",
      "  847/5000: episode: 87, duration: 0.017s, episode steps: 12, steps per second: 695, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.122 [-1.907, 3.024], loss: 0.263552, mean_absolute_error: 2.027477, mean_q: 4.711137\n",
      "  857/5000: episode: 88, duration: 0.014s, episode steps: 10, steps per second: 702, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [-1.988, 3.026], loss: 0.269288, mean_absolute_error: 2.050518, mean_q: 4.746314\n",
      "  867/5000: episode: 89, duration: 0.014s, episode steps: 10, steps per second: 694, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.133 [-1.591, 2.513], loss: 0.232187, mean_absolute_error: 2.070181, mean_q: 4.790903\n",
      "  878/5000: episode: 90, duration: 0.016s, episode steps: 11, steps per second: 693, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.112 [-1.719, 2.730], loss: 0.217434, mean_absolute_error: 2.115466, mean_q: 4.866410\n",
      "  891/5000: episode: 91, duration: 0.018s, episode steps: 13, steps per second: 703, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.077 [0.000, 1.000], mean observation: 0.114 [-2.097, 3.251], loss: 0.175309, mean_absolute_error: 2.148296, mean_q: 5.082668\n",
      "  901/5000: episode: 92, duration: 0.014s, episode steps: 10, steps per second: 694, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.977, 3.044], loss: 0.195197, mean_absolute_error: 2.208167, mean_q: 5.060630\n",
      "  911/5000: episode: 93, duration: 0.014s, episode steps: 10, steps per second: 699, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [-1.956, 3.023], loss: 0.206450, mean_absolute_error: 2.126042, mean_q: 4.787690\n",
      "  921/5000: episode: 94, duration: 0.015s, episode steps: 10, steps per second: 688, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.132 [-1.588, 2.513], loss: 0.136649, mean_absolute_error: 2.212759, mean_q: 5.113379\n",
      "  929/5000: episode: 95, duration: 0.012s, episode steps: 8, steps per second: 675, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.172 [-1.532, 2.606], loss: 0.174702, mean_absolute_error: 2.201764, mean_q: 4.965735\n",
      "  940/5000: episode: 96, duration: 0.016s, episode steps: 11, steps per second: 673, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.146 [-1.716, 2.837], loss: 0.164635, mean_absolute_error: 2.226118, mean_q: 5.022226\n",
      "  950/5000: episode: 97, duration: 0.017s, episode steps: 10, steps per second: 585, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.126 [-1.967, 3.026], loss: 0.121788, mean_absolute_error: 2.224617, mean_q: 5.000968\n",
      "  959/5000: episode: 98, duration: 0.014s, episode steps: 9, steps per second: 644, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.147 [-1.346, 2.332], loss: 0.150240, mean_absolute_error: 2.390721, mean_q: 5.393991\n",
      "  968/5000: episode: 99, duration: 0.013s, episode steps: 9, steps per second: 678, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-1.772, 2.855], loss: 0.148516, mean_absolute_error: 2.311133, mean_q: 5.119702\n",
      "  978/5000: episode: 100, duration: 0.015s, episode steps: 10, steps per second: 689, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.106 [-1.799, 2.666], loss: 0.125026, mean_absolute_error: 2.361394, mean_q: 5.270625\n",
      "  990/5000: episode: 101, duration: 0.018s, episode steps: 12, steps per second: 683, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.099 [-1.750, 2.679], loss: 0.144926, mean_absolute_error: 2.353710, mean_q: 5.167837\n",
      " 1000/5000: episode: 102, duration: 0.014s, episode steps: 10, steps per second: 699, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.121 [-1.767, 2.636], loss: 0.105049, mean_absolute_error: 2.409616, mean_q: 5.321921\n",
      " 1010/5000: episode: 103, duration: 0.015s, episode steps: 10, steps per second: 688, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.135 [-1.748, 2.659], loss: 0.126762, mean_absolute_error: 2.435376, mean_q: 5.308244\n",
      " 1020/5000: episode: 104, duration: 0.015s, episode steps: 10, steps per second: 679, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.129 [-1.567, 2.588], loss: 0.116835, mean_absolute_error: 2.401858, mean_q: 5.216977\n",
      " 1032/5000: episode: 105, duration: 0.017s, episode steps: 12, steps per second: 703, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.109 [-1.775, 2.693], loss: 0.107065, mean_absolute_error: 2.557219, mean_q: 5.561623\n",
      " 1042/5000: episode: 106, duration: 0.015s, episode steps: 10, steps per second: 689, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.128 [-1.765, 2.700], loss: 0.094976, mean_absolute_error: 2.447918, mean_q: 5.288199\n",
      " 1053/5000: episode: 107, duration: 0.016s, episode steps: 11, steps per second: 680, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.146 [-1.551, 2.495], loss: 0.085944, mean_absolute_error: 2.630525, mean_q: 5.700108\n",
      " 1063/5000: episode: 108, duration: 0.015s, episode steps: 10, steps per second: 685, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.166 [-1.716, 2.756], loss: 0.100018, mean_absolute_error: 2.652411, mean_q: 5.692505\n",
      " 1071/5000: episode: 109, duration: 0.012s, episode steps: 8, steps per second: 684, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [-1.534, 2.594], loss: 0.097328, mean_absolute_error: 2.556513, mean_q: 5.442802\n",
      " 1082/5000: episode: 110, duration: 0.016s, episode steps: 11, steps per second: 675, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.107 [-1.563, 2.469], loss: 0.107748, mean_absolute_error: 2.535675, mean_q: 5.334630\n",
      " 1091/5000: episode: 111, duration: 0.015s, episode steps: 9, steps per second: 608, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.124 [-1.576, 2.445], loss: 0.080095, mean_absolute_error: 2.684813, mean_q: 5.677515\n",
      " 1101/5000: episode: 112, duration: 0.015s, episode steps: 10, steps per second: 655, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.127 [-1.542, 2.356], loss: 0.087152, mean_absolute_error: 2.627752, mean_q: 5.477796\n",
      " 1113/5000: episode: 113, duration: 0.018s, episode steps: 12, steps per second: 662, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.132 [-1.347, 2.231], loss: 0.081457, mean_absolute_error: 2.725515, mean_q: 5.666754\n",
      " 1123/5000: episode: 114, duration: 0.017s, episode steps: 10, steps per second: 600, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.112 [-1.211, 1.938], loss: 0.092923, mean_absolute_error: 2.711523, mean_q: 5.574946\n",
      " 1132/5000: episode: 115, duration: 0.013s, episode steps: 9, steps per second: 667, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.127 [-1.405, 2.159], loss: 0.076782, mean_absolute_error: 2.615562, mean_q: 5.372888\n",
      " 1141/5000: episode: 116, duration: 0.013s, episode steps: 9, steps per second: 673, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.150 [-1.399, 2.198], loss: 0.087053, mean_absolute_error: 2.699735, mean_q: 5.519755\n",
      " 1154/5000: episode: 117, duration: 0.020s, episode steps: 13, steps per second: 664, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.089 [-1.550, 2.266], loss: 0.097188, mean_absolute_error: 2.795591, mean_q: 5.679720\n",
      " 1165/5000: episode: 118, duration: 0.016s, episode steps: 11, steps per second: 693, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.107 [-1.391, 2.037], loss: 0.072908, mean_absolute_error: 2.793261, mean_q: 5.685273\n",
      " 1177/5000: episode: 119, duration: 0.017s, episode steps: 12, steps per second: 695, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.094 [-1.354, 1.982], loss: 0.072022, mean_absolute_error: 2.742331, mean_q: 5.542093\n",
      " 1189/5000: episode: 120, duration: 0.017s, episode steps: 12, steps per second: 709, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.110 [-1.320, 1.985], loss: 0.071565, mean_absolute_error: 2.816273, mean_q: 5.664393\n",
      " 1202/5000: episode: 121, duration: 0.018s, episode steps: 13, steps per second: 708, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.069 [-1.402, 2.025], loss: 0.083525, mean_absolute_error: 2.838486, mean_q: 5.676794\n",
      " 1211/5000: episode: 122, duration: 0.013s, episode steps: 9, steps per second: 670, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.141 [-1.187, 1.848], loss: 0.096680, mean_absolute_error: 2.811610, mean_q: 5.570898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1223/5000: episode: 123, duration: 0.019s, episode steps: 12, steps per second: 634, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.079 [-1.209, 1.760], loss: 0.079413, mean_absolute_error: 2.830729, mean_q: 5.604668\n",
      " 1234/5000: episode: 124, duration: 0.017s, episode steps: 11, steps per second: 666, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.106 [-0.979, 1.598], loss: 0.108560, mean_absolute_error: 2.896635, mean_q: 5.688229\n",
      " 1245/5000: episode: 125, duration: 0.016s, episode steps: 11, steps per second: 679, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.112 [-1.176, 1.788], loss: 0.099689, mean_absolute_error: 2.894938, mean_q: 5.681004\n",
      " 1260/5000: episode: 126, duration: 0.021s, episode steps: 15, steps per second: 704, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.075 [-1.140, 1.666], loss: 0.087503, mean_absolute_error: 2.851247, mean_q: 5.581380\n",
      " 1276/5000: episode: 127, duration: 0.023s, episode steps: 16, steps per second: 710, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-1.134, 1.682], loss: 0.097128, mean_absolute_error: 3.003154, mean_q: 5.854969\n",
      " 1297/5000: episode: 128, duration: 0.029s, episode steps: 21, steps per second: 727, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.098 [-0.736, 1.228], loss: 0.091285, mean_absolute_error: 3.139462, mean_q: 6.111752\n",
      " 1331/5000: episode: 129, duration: 0.046s, episode steps: 34, steps per second: 742, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.010 [-1.351, 1.134], loss: 0.101061, mean_absolute_error: 3.136789, mean_q: 6.051891\n",
      " 1342/5000: episode: 130, duration: 0.016s, episode steps: 11, steps per second: 708, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.119 [-2.431, 1.543], loss: 0.108939, mean_absolute_error: 3.258974, mean_q: 6.299763\n",
      " 1351/5000: episode: 131, duration: 0.014s, episode steps: 9, steps per second: 666, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-2.789, 1.742], loss: 0.093595, mean_absolute_error: 3.312915, mean_q: 6.452136\n",
      " 1361/5000: episode: 132, duration: 0.016s, episode steps: 10, steps per second: 627, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.122 [-3.045, 1.969], loss: 0.348843, mean_absolute_error: 3.361806, mean_q: 6.496490\n",
      " 1371/5000: episode: 133, duration: 0.015s, episode steps: 10, steps per second: 665, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.118 [-2.479, 1.596], loss: 0.081688, mean_absolute_error: 3.262489, mean_q: 6.356637\n",
      " 1382/5000: episode: 134, duration: 0.016s, episode steps: 11, steps per second: 694, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.131 [-2.518, 1.589], loss: 0.284898, mean_absolute_error: 3.365198, mean_q: 6.524017\n",
      " 1391/5000: episode: 135, duration: 0.013s, episode steps: 9, steps per second: 669, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.167 [-2.909, 1.748], loss: 0.285201, mean_absolute_error: 3.276919, mean_q: 6.364611\n",
      " 1403/5000: episode: 136, duration: 0.017s, episode steps: 12, steps per second: 693, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.117 [-3.030, 1.912], loss: 0.307553, mean_absolute_error: 3.332622, mean_q: 6.435501\n",
      " 1412/5000: episode: 137, duration: 0.013s, episode steps: 9, steps per second: 681, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-2.860, 1.766], loss: 0.377308, mean_absolute_error: 3.584455, mean_q: 6.933497\n",
      " 1421/5000: episode: 138, duration: 0.013s, episode steps: 9, steps per second: 683, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.135 [-2.458, 1.587], loss: 0.688534, mean_absolute_error: 3.614657, mean_q: 6.916201\n",
      " 1431/5000: episode: 139, duration: 0.014s, episode steps: 10, steps per second: 702, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-3.064, 1.910], loss: 0.577813, mean_absolute_error: 3.566559, mean_q: 6.826354\n",
      " 1439/5000: episode: 140, duration: 0.012s, episode steps: 8, steps per second: 673, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-2.509, 1.570], loss: 0.233189, mean_absolute_error: 3.474368, mean_q: 6.649279\n",
      " 1449/5000: episode: 141, duration: 0.015s, episode steps: 10, steps per second: 682, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.154 [-3.102, 1.935], loss: 0.797254, mean_absolute_error: 3.608941, mean_q: 6.834826\n",
      " 1457/5000: episode: 142, duration: 0.012s, episode steps: 8, steps per second: 663, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.143 [-1.911, 1.163], loss: 1.122066, mean_absolute_error: 3.737144, mean_q: 7.006984\n",
      " 1481/5000: episode: 143, duration: 0.033s, episode steps: 24, steps per second: 722, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.036 [-2.661, 1.620], loss: 0.812060, mean_absolute_error: 3.640329, mean_q: 6.829323\n",
      " 1491/5000: episode: 144, duration: 0.015s, episode steps: 10, steps per second: 670, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-3.070, 1.933], loss: 0.963200, mean_absolute_error: 3.674497, mean_q: 6.859693\n",
      " 1504/5000: episode: 145, duration: 0.020s, episode steps: 13, steps per second: 647, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.106 [-2.806, 1.782], loss: 1.338593, mean_absolute_error: 3.799426, mean_q: 7.038650\n",
      " 1512/5000: episode: 146, duration: 0.013s, episode steps: 8, steps per second: 618, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.171 [-2.586, 1.548], loss: 1.534054, mean_absolute_error: 3.851606, mean_q: 7.002239\n",
      " 1524/5000: episode: 147, duration: 0.018s, episode steps: 12, steps per second: 685, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.123 [-2.551, 1.533], loss: 1.027362, mean_absolute_error: 3.814559, mean_q: 7.082172\n",
      " 1534/5000: episode: 148, duration: 0.015s, episode steps: 10, steps per second: 674, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.115 [-2.476, 1.568], loss: 0.899083, mean_absolute_error: 3.989129, mean_q: 7.436604\n",
      " 1542/5000: episode: 149, duration: 0.012s, episode steps: 8, steps per second: 672, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.139 [-2.552, 1.571], loss: 1.202934, mean_absolute_error: 3.976068, mean_q: 7.442738\n",
      " 1551/5000: episode: 150, duration: 0.014s, episode steps: 9, steps per second: 637, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.164 [-2.828, 1.751], loss: 1.056769, mean_absolute_error: 3.945924, mean_q: 7.360648\n",
      " 1562/5000: episode: 151, duration: 0.017s, episode steps: 11, steps per second: 660, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.115 [-2.251, 1.374], loss: 0.811461, mean_absolute_error: 3.816348, mean_q: 7.193767\n",
      " 1572/5000: episode: 152, duration: 0.016s, episode steps: 10, steps per second: 628, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.164 [-3.058, 1.908], loss: 1.821685, mean_absolute_error: 3.923854, mean_q: 7.234263\n",
      " 1581/5000: episode: 153, duration: 0.014s, episode steps: 9, steps per second: 658, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-2.752, 1.767], loss: 1.038313, mean_absolute_error: 3.788007, mean_q: 7.046903\n",
      " 1592/5000: episode: 154, duration: 0.017s, episode steps: 11, steps per second: 637, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.141 [-2.873, 1.739], loss: 1.478668, mean_absolute_error: 3.928677, mean_q: 7.235556\n",
      " 1602/5000: episode: 155, duration: 0.018s, episode steps: 10, steps per second: 568, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-3.103, 1.967], loss: 0.879606, mean_absolute_error: 4.061190, mean_q: 7.581143\n",
      " 1611/5000: episode: 156, duration: 0.014s, episode steps: 9, steps per second: 641, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.154 [-2.490, 1.547], loss: 0.735058, mean_absolute_error: 4.069147, mean_q: 7.669991\n",
      " 1624/5000: episode: 157, duration: 0.019s, episode steps: 13, steps per second: 681, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.107 [-2.700, 1.737], loss: 1.149994, mean_absolute_error: 4.103630, mean_q: 7.698069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1633/5000: episode: 158, duration: 0.014s, episode steps: 9, steps per second: 623, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-2.813, 1.769], loss: 0.780561, mean_absolute_error: 4.024928, mean_q: 7.621020\n",
      " 1643/5000: episode: 159, duration: 0.016s, episode steps: 10, steps per second: 631, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.116 [-2.646, 1.786], loss: 1.115473, mean_absolute_error: 4.029023, mean_q: 7.576444\n",
      " 1651/5000: episode: 160, duration: 0.013s, episode steps: 8, steps per second: 617, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.129 [-2.510, 1.617], loss: 1.172853, mean_absolute_error: 4.180949, mean_q: 7.808504\n",
      " 1663/5000: episode: 161, duration: 0.018s, episode steps: 12, steps per second: 672, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.092 [-2.633, 1.747], loss: 0.855718, mean_absolute_error: 3.979863, mean_q: 7.500026\n",
      " 1674/5000: episode: 162, duration: 0.016s, episode steps: 11, steps per second: 680, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.107 [-2.730, 1.780], loss: 0.809777, mean_absolute_error: 4.127429, mean_q: 7.802627\n",
      " 1684/5000: episode: 163, duration: 0.016s, episode steps: 10, steps per second: 641, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.114 [-2.502, 1.528], loss: 1.246538, mean_absolute_error: 4.177365, mean_q: 7.808183\n",
      " 1694/5000: episode: 164, duration: 0.015s, episode steps: 10, steps per second: 669, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.110 [-2.991, 1.976], loss: 1.316540, mean_absolute_error: 4.123079, mean_q: 7.664990\n",
      " 1705/5000: episode: 165, duration: 0.019s, episode steps: 11, steps per second: 591, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.139 [-2.820, 1.736], loss: 1.316056, mean_absolute_error: 4.179766, mean_q: 7.806389\n",
      " 1715/5000: episode: 166, duration: 0.016s, episode steps: 10, steps per second: 641, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.139 [-3.012, 1.904], loss: 1.344631, mean_absolute_error: 4.175335, mean_q: 7.802271\n",
      " 1727/5000: episode: 167, duration: 0.018s, episode steps: 12, steps per second: 659, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.117 [-3.056, 1.966], loss: 1.446314, mean_absolute_error: 4.300992, mean_q: 7.976479\n",
      " 1736/5000: episode: 168, duration: 0.014s, episode steps: 9, steps per second: 648, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-2.810, 1.803], loss: 1.556799, mean_absolute_error: 4.434279, mean_q: 8.206279\n",
      " 1745/5000: episode: 169, duration: 0.014s, episode steps: 9, steps per second: 644, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.170 [-2.883, 1.756], loss: 1.471110, mean_absolute_error: 4.032793, mean_q: 7.441690\n",
      " 1754/5000: episode: 170, duration: 0.014s, episode steps: 9, steps per second: 634, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-2.788, 1.781], loss: 1.119781, mean_absolute_error: 4.331012, mean_q: 8.074050\n",
      " 1766/5000: episode: 171, duration: 0.020s, episode steps: 12, steps per second: 596, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.133 [-3.114, 1.919], loss: 0.985278, mean_absolute_error: 4.230461, mean_q: 7.987478\n",
      " 1776/5000: episode: 172, duration: 0.016s, episode steps: 10, steps per second: 633, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.126 [-3.072, 1.993], loss: 0.629359, mean_absolute_error: 4.033364, mean_q: 7.726195\n",
      " 1784/5000: episode: 173, duration: 0.013s, episode steps: 8, steps per second: 635, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-2.609, 1.614], loss: 1.089123, mean_absolute_error: 4.199456, mean_q: 7.933628\n",
      " 1796/5000: episode: 174, duration: 0.018s, episode steps: 12, steps per second: 656, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.126 [-3.000, 1.901], loss: 1.731104, mean_absolute_error: 4.393001, mean_q: 8.221872\n",
      " 1806/5000: episode: 175, duration: 0.015s, episode steps: 10, steps per second: 650, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.124 [-2.973, 1.977], loss: 0.865441, mean_absolute_error: 4.309871, mean_q: 8.159346\n",
      " 1815/5000: episode: 176, duration: 0.014s, episode steps: 9, steps per second: 629, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.180 [-2.904, 1.742], loss: 1.096825, mean_absolute_error: 4.402345, mean_q: 8.274083\n",
      " 1824/5000: episode: 177, duration: 0.014s, episode steps: 9, steps per second: 663, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-2.842, 1.779], loss: 0.523507, mean_absolute_error: 4.299177, mean_q: 8.230406\n",
      " 1836/5000: episode: 178, duration: 0.018s, episode steps: 12, steps per second: 672, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.120 [-3.048, 1.940], loss: 0.761744, mean_absolute_error: 4.213738, mean_q: 8.069446\n",
      " 1847/5000: episode: 179, duration: 0.017s, episode steps: 11, steps per second: 660, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.130 [-2.785, 1.757], loss: 1.289451, mean_absolute_error: 4.407547, mean_q: 8.427752\n",
      " 1857/5000: episode: 180, duration: 0.016s, episode steps: 10, steps per second: 636, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.128 [-2.641, 1.585], loss: 1.100121, mean_absolute_error: 4.446696, mean_q: 8.456717\n",
      " 1866/5000: episode: 181, duration: 0.014s, episode steps: 9, steps per second: 665, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.130 [-2.457, 1.591], loss: 0.942110, mean_absolute_error: 4.313633, mean_q: 8.181732\n",
      " 1875/5000: episode: 182, duration: 0.014s, episode steps: 9, steps per second: 638, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.160 [-2.814, 1.726], loss: 1.026682, mean_absolute_error: 4.374434, mean_q: 8.296868\n",
      " 1883/5000: episode: 183, duration: 0.013s, episode steps: 8, steps per second: 633, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-2.533, 1.550], loss: 1.026098, mean_absolute_error: 4.529360, mean_q: 8.611155\n",
      " 1891/5000: episode: 184, duration: 0.014s, episode steps: 8, steps per second: 564, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.163 [-2.559, 1.518], loss: 1.185080, mean_absolute_error: 4.473722, mean_q: 8.431684\n",
      " 1901/5000: episode: 185, duration: 0.016s, episode steps: 10, steps per second: 633, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.129 [-3.062, 1.971], loss: 0.893879, mean_absolute_error: 4.220436, mean_q: 7.993867\n",
      " 1911/5000: episode: 186, duration: 0.015s, episode steps: 10, steps per second: 668, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-3.050, 1.952], loss: 0.748946, mean_absolute_error: 4.377278, mean_q: 8.357336\n",
      " 1921/5000: episode: 187, duration: 0.015s, episode steps: 10, steps per second: 668, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.130 [-2.584, 1.576], loss: 0.700738, mean_absolute_error: 4.329427, mean_q: 8.273626\n",
      " 1931/5000: episode: 188, duration: 0.016s, episode steps: 10, steps per second: 644, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.153 [-3.092, 1.915], loss: 1.651065, mean_absolute_error: 4.422090, mean_q: 8.233116\n",
      " 1942/5000: episode: 189, duration: 0.016s, episode steps: 11, steps per second: 667, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.107 [-2.718, 1.740], loss: 0.822036, mean_absolute_error: 4.309751, mean_q: 8.176246\n",
      " 1953/5000: episode: 190, duration: 0.018s, episode steps: 11, steps per second: 625, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.140 [-2.847, 1.785], loss: 1.231404, mean_absolute_error: 4.493701, mean_q: 8.409403\n",
      " 1967/5000: episode: 191, duration: 0.021s, episode steps: 14, steps per second: 654, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.087 [-2.494, 1.567], loss: 0.790356, mean_absolute_error: 4.388934, mean_q: 8.320007\n",
      " 1976/5000: episode: 192, duration: 0.014s, episode steps: 9, steps per second: 648, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-2.837, 1.772], loss: 1.026000, mean_absolute_error: 4.447829, mean_q: 8.397261\n",
      " 1984/5000: episode: 193, duration: 0.012s, episode steps: 8, steps per second: 641, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-2.548, 1.551], loss: 1.036257, mean_absolute_error: 4.498818, mean_q: 8.507137\n",
      " 1994/5000: episode: 194, duration: 0.015s, episode steps: 10, steps per second: 669, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.140 [-2.196, 1.347], loss: 0.964245, mean_absolute_error: 4.586496, mean_q: 8.714942\n",
      " 2003/5000: episode: 195, duration: 0.014s, episode steps: 9, steps per second: 648, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.154 [-2.909, 1.812], loss: 0.571439, mean_absolute_error: 4.381304, mean_q: 8.401243\n",
      " 2013/5000: episode: 196, duration: 0.015s, episode steps: 10, steps per second: 680, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.118 [-2.993, 1.999], loss: 1.224195, mean_absolute_error: 4.497819, mean_q: 8.492064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2024/5000: episode: 197, duration: 0.019s, episode steps: 11, steps per second: 567, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.124 [-2.829, 1.739], loss: 0.826796, mean_absolute_error: 4.616831, mean_q: 8.844901\n",
      " 2034/5000: episode: 198, duration: 0.016s, episode steps: 10, steps per second: 637, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-3.065, 1.927], loss: 0.782135, mean_absolute_error: 4.509972, mean_q: 8.657809\n",
      " 2045/5000: episode: 199, duration: 0.017s, episode steps: 11, steps per second: 667, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.104 [-2.733, 1.785], loss: 1.124796, mean_absolute_error: 4.540173, mean_q: 8.621413\n",
      " 2056/5000: episode: 200, duration: 0.017s, episode steps: 11, steps per second: 645, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.122 [-2.825, 1.732], loss: 1.050986, mean_absolute_error: 4.520377, mean_q: 8.604409\n",
      " 2066/5000: episode: 201, duration: 0.015s, episode steps: 10, steps per second: 660, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.125 [-3.034, 1.983], loss: 0.808887, mean_absolute_error: 4.233079, mean_q: 8.039299\n",
      " 2076/5000: episode: 202, duration: 0.015s, episode steps: 10, steps per second: 663, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.146 [-2.536, 1.519], loss: 0.731637, mean_absolute_error: 4.535611, mean_q: 8.632457\n",
      " 2088/5000: episode: 203, duration: 0.017s, episode steps: 12, steps per second: 688, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.118 [-3.066, 1.949], loss: 0.704350, mean_absolute_error: 4.417261, mean_q: 8.406960\n",
      " 2099/5000: episode: 204, duration: 0.016s, episode steps: 11, steps per second: 670, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.104 [-2.833, 1.809], loss: 0.790169, mean_absolute_error: 4.505391, mean_q: 8.628256\n",
      " 2109/5000: episode: 205, duration: 0.016s, episode steps: 10, steps per second: 642, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.129 [-2.998, 1.930], loss: 0.687210, mean_absolute_error: 4.425392, mean_q: 8.511709\n",
      " 2119/5000: episode: 206, duration: 0.015s, episode steps: 10, steps per second: 671, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.119 [-3.070, 2.002], loss: 0.591105, mean_absolute_error: 4.460444, mean_q: 8.540223\n",
      " 2129/5000: episode: 207, duration: 0.016s, episode steps: 10, steps per second: 639, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-3.081, 1.934], loss: 0.713286, mean_absolute_error: 4.540071, mean_q: 8.677996\n",
      " 2138/5000: episode: 208, duration: 0.013s, episode steps: 9, steps per second: 668, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-2.833, 1.756], loss: 0.804925, mean_absolute_error: 4.626268, mean_q: 8.792315\n",
      " 2147/5000: episode: 209, duration: 0.013s, episode steps: 9, steps per second: 671, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-2.846, 1.806], loss: 0.907935, mean_absolute_error: 4.529429, mean_q: 8.542583\n",
      " 2157/5000: episode: 210, duration: 0.018s, episode steps: 10, steps per second: 570, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-3.017, 1.906], loss: 0.914875, mean_absolute_error: 4.548666, mean_q: 8.631564\n",
      " 2168/5000: episode: 211, duration: 0.018s, episode steps: 11, steps per second: 624, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.128 [-2.842, 1.726], loss: 0.760699, mean_absolute_error: 4.425427, mean_q: 8.382376\n",
      " 2178/5000: episode: 212, duration: 0.015s, episode steps: 10, steps per second: 661, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-3.007, 1.912], loss: 0.765435, mean_absolute_error: 4.457213, mean_q: 8.404588\n",
      " 2187/5000: episode: 213, duration: 0.014s, episode steps: 9, steps per second: 642, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-2.814, 1.794], loss: 0.661107, mean_absolute_error: 4.465321, mean_q: 8.539843\n",
      " 2197/5000: episode: 214, duration: 0.015s, episode steps: 10, steps per second: 671, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-3.052, 1.968], loss: 0.623707, mean_absolute_error: 4.516313, mean_q: 8.657954\n",
      " 2208/5000: episode: 215, duration: 0.017s, episode steps: 11, steps per second: 665, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.106 [-2.802, 1.791], loss: 0.889174, mean_absolute_error: 4.477543, mean_q: 8.505245\n",
      " 2218/5000: episode: 216, duration: 0.015s, episode steps: 10, steps per second: 658, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.128 [-3.013, 1.953], loss: 0.779672, mean_absolute_error: 4.494893, mean_q: 8.580748\n",
      " 2230/5000: episode: 217, duration: 0.018s, episode steps: 12, steps per second: 668, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.130 [-3.117, 1.935], loss: 0.836402, mean_absolute_error: 4.405074, mean_q: 8.349751\n",
      " 2242/5000: episode: 218, duration: 0.018s, episode steps: 12, steps per second: 680, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.131 [-2.630, 1.525], loss: 0.666107, mean_absolute_error: 4.591661, mean_q: 8.750945\n",
      " 2251/5000: episode: 219, duration: 0.014s, episode steps: 9, steps per second: 654, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.162 [-2.895, 1.779], loss: 0.699460, mean_absolute_error: 4.421478, mean_q: 8.425077\n",
      " 2262/5000: episode: 220, duration: 0.016s, episode steps: 11, steps per second: 667, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.125 [-2.843, 1.768], loss: 0.687209, mean_absolute_error: 4.583088, mean_q: 8.771720\n",
      " 2272/5000: episode: 221, duration: 0.015s, episode steps: 10, steps per second: 673, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-3.050, 1.915], loss: 0.545761, mean_absolute_error: 4.504497, mean_q: 8.635823\n",
      " 2282/5000: episode: 222, duration: 0.015s, episode steps: 10, steps per second: 655, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-3.107, 1.942], loss: 0.679884, mean_absolute_error: 4.547847, mean_q: 8.708195\n",
      " 2291/5000: episode: 223, duration: 0.015s, episode steps: 9, steps per second: 581, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.122 [-2.785, 1.803], loss: 0.540200, mean_absolute_error: 4.344512, mean_q: 8.349769\n",
      " 2301/5000: episode: 224, duration: 0.015s, episode steps: 10, steps per second: 647, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.169 [-3.119, 1.934], loss: 0.662613, mean_absolute_error: 4.265127, mean_q: 8.179407\n",
      " 2309/5000: episode: 225, duration: 0.012s, episode steps: 8, steps per second: 655, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-2.509, 1.615], loss: 0.820255, mean_absolute_error: 4.301935, mean_q: 8.121686\n",
      " 2320/5000: episode: 226, duration: 0.016s, episode steps: 11, steps per second: 672, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-3.311, 2.147], loss: 0.697480, mean_absolute_error: 4.475374, mean_q: 8.525367\n",
      " 2330/5000: episode: 227, duration: 0.015s, episode steps: 10, steps per second: 678, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.145 [-2.660, 1.599], loss: 0.534952, mean_absolute_error: 4.430001, mean_q: 8.501683\n",
      " 2341/5000: episode: 228, duration: 0.016s, episode steps: 11, steps per second: 679, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.110 [-2.282, 1.360], loss: 0.763506, mean_absolute_error: 4.411775, mean_q: 8.396805\n",
      " 2350/5000: episode: 229, duration: 0.013s, episode steps: 9, steps per second: 673, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.134 [-2.488, 1.586], loss: 0.760090, mean_absolute_error: 4.541842, mean_q: 8.604877\n",
      " 2359/5000: episode: 230, duration: 0.013s, episode steps: 9, steps per second: 668, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.177 [-2.880, 1.725], loss: 0.544507, mean_absolute_error: 4.478647, mean_q: 8.536909\n",
      " 2367/5000: episode: 231, duration: 0.012s, episode steps: 8, steps per second: 661, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.160 [-2.582, 1.519], loss: 0.642397, mean_absolute_error: 4.491781, mean_q: 8.589436\n",
      " 2377/5000: episode: 232, duration: 0.015s, episode steps: 10, steps per second: 665, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.106 [-2.983, 1.982], loss: 0.791254, mean_absolute_error: 4.408632, mean_q: 8.418400\n",
      " 2387/5000: episode: 233, duration: 0.015s, episode steps: 10, steps per second: 678, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.147 [-2.097, 1.142], loss: 0.491310, mean_absolute_error: 4.469444, mean_q: 8.592775\n",
      " 2397/5000: episode: 234, duration: 0.015s, episode steps: 10, steps per second: 670, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.127 [-3.003, 1.945], loss: 0.557611, mean_absolute_error: 4.398242, mean_q: 8.424639\n",
      " 2407/5000: episode: 235, duration: 0.015s, episode steps: 10, steps per second: 674, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.114 [-2.546, 1.586], loss: 0.630047, mean_absolute_error: 4.372072, mean_q: 8.315487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2417/5000: episode: 236, duration: 0.017s, episode steps: 10, steps per second: 605, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-3.053, 1.905], loss: 0.536041, mean_absolute_error: 4.440173, mean_q: 8.516920\n",
      " 2429/5000: episode: 237, duration: 0.022s, episode steps: 12, steps per second: 555, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.101 [-3.025, 1.980], loss: 0.698722, mean_absolute_error: 4.548768, mean_q: 8.700689\n",
      " 2440/5000: episode: 238, duration: 0.016s, episode steps: 11, steps per second: 670, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.155 [-3.316, 2.104], loss: 0.399125, mean_absolute_error: 4.410175, mean_q: 8.529565\n",
      " 2450/5000: episode: 239, duration: 0.015s, episode steps: 10, steps per second: 680, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.160 [-3.132, 1.924], loss: 0.303678, mean_absolute_error: 4.444706, mean_q: 8.706674\n",
      " 2458/5000: episode: 240, duration: 0.012s, episode steps: 8, steps per second: 644, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.139 [-2.228, 1.375], loss: 0.497918, mean_absolute_error: 4.087869, mean_q: 7.800334\n",
      " 2467/5000: episode: 241, duration: 0.014s, episode steps: 9, steps per second: 662, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-2.832, 1.756], loss: 0.360992, mean_absolute_error: 4.393352, mean_q: 8.512232\n",
      " 2477/5000: episode: 242, duration: 0.015s, episode steps: 10, steps per second: 680, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.150 [-2.568, 1.520], loss: 0.536083, mean_absolute_error: 4.298401, mean_q: 8.223867\n",
      " 2486/5000: episode: 243, duration: 0.013s, episode steps: 9, steps per second: 667, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.124 [-2.807, 1.803], loss: 0.480147, mean_absolute_error: 4.307517, mean_q: 8.246551\n",
      " 2496/5000: episode: 244, duration: 0.015s, episode steps: 10, steps per second: 672, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.171 [-3.106, 1.918], loss: 0.428053, mean_absolute_error: 4.257941, mean_q: 8.173559\n",
      " 2506/5000: episode: 245, duration: 0.015s, episode steps: 10, steps per second: 658, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.154 [-3.095, 1.905], loss: 0.304881, mean_absolute_error: 4.232482, mean_q: 8.209397\n",
      " 2516/5000: episode: 246, duration: 0.015s, episode steps: 10, steps per second: 666, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-3.034, 1.914], loss: 0.584985, mean_absolute_error: 4.430121, mean_q: 8.478660\n",
      " 2529/5000: episode: 247, duration: 0.019s, episode steps: 13, steps per second: 686, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.103 [-2.814, 1.784], loss: 0.558302, mean_absolute_error: 4.217233, mean_q: 8.024897\n",
      " 2539/5000: episode: 248, duration: 0.015s, episode steps: 10, steps per second: 658, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-3.011, 1.912], loss: 0.375181, mean_absolute_error: 4.259741, mean_q: 8.200994\n",
      " 2550/5000: episode: 249, duration: 0.018s, episode steps: 11, steps per second: 626, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.150 [-2.372, 1.340], loss: 0.420504, mean_absolute_error: 4.422956, mean_q: 8.470782\n",
      " 2558/5000: episode: 250, duration: 0.013s, episode steps: 8, steps per second: 633, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.134 [-2.532, 1.588], loss: 0.362594, mean_absolute_error: 4.307006, mean_q: 8.304090\n",
      " 2571/5000: episode: 251, duration: 0.019s, episode steps: 13, steps per second: 686, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.923 [0.000, 1.000], mean observation: -0.091 [-3.261, 2.157], loss: 0.464170, mean_absolute_error: 4.265676, mean_q: 8.184099\n",
      " 2579/5000: episode: 252, duration: 0.012s, episode steps: 8, steps per second: 657, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-2.516, 1.561], loss: 0.345339, mean_absolute_error: 4.122687, mean_q: 7.942246\n",
      " 2589/5000: episode: 253, duration: 0.015s, episode steps: 10, steps per second: 681, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.112 [-3.028, 2.001], loss: 0.354769, mean_absolute_error: 4.240572, mean_q: 8.182814\n",
      " 2602/5000: episode: 254, duration: 0.019s, episode steps: 13, steps per second: 700, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.923 [0.000, 1.000], mean observation: -0.101 [-3.272, 2.140], loss: 0.381652, mean_absolute_error: 4.195761, mean_q: 8.116875\n",
      " 2611/5000: episode: 255, duration: 0.014s, episode steps: 9, steps per second: 657, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-2.763, 1.793], loss: 0.321518, mean_absolute_error: 4.236190, mean_q: 8.177260\n",
      " 2622/5000: episode: 256, duration: 0.016s, episode steps: 11, steps per second: 684, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.129 [-2.839, 1.794], loss: 0.352632, mean_absolute_error: 4.135964, mean_q: 7.959816\n",
      " 2635/5000: episode: 257, duration: 0.021s, episode steps: 13, steps per second: 632, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.096 [-2.742, 1.796], loss: 0.299174, mean_absolute_error: 4.292710, mean_q: 8.293585\n",
      " 2645/5000: episode: 258, duration: 0.015s, episode steps: 10, steps per second: 659, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-3.077, 1.973], loss: 0.376601, mean_absolute_error: 4.168136, mean_q: 7.955173\n",
      " 2654/5000: episode: 259, duration: 0.013s, episode steps: 9, steps per second: 670, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-2.772, 1.751], loss: 0.591123, mean_absolute_error: 4.251613, mean_q: 8.045877\n",
      " 2663/5000: episode: 260, duration: 0.013s, episode steps: 9, steps per second: 677, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-2.821, 1.745], loss: 0.466038, mean_absolute_error: 4.155764, mean_q: 7.948125\n",
      " 2673/5000: episode: 261, duration: 0.015s, episode steps: 10, steps per second: 657, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.124 [-2.986, 1.937], loss: 0.444012, mean_absolute_error: 4.243937, mean_q: 8.161508\n",
      " 2683/5000: episode: 262, duration: 0.017s, episode steps: 10, steps per second: 584, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-3.045, 1.927], loss: 0.374867, mean_absolute_error: 4.157441, mean_q: 7.968422\n",
      " 2693/5000: episode: 263, duration: 0.015s, episode steps: 10, steps per second: 666, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-2.979, 1.947], loss: 0.238103, mean_absolute_error: 4.008201, mean_q: 7.741735\n",
      " 2701/5000: episode: 264, duration: 0.012s, episode steps: 8, steps per second: 658, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-2.570, 1.549], loss: 0.318870, mean_absolute_error: 4.090325, mean_q: 7.851244\n",
      " 2711/5000: episode: 265, duration: 0.015s, episode steps: 10, steps per second: 675, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-3.033, 1.904], loss: 0.535376, mean_absolute_error: 4.215551, mean_q: 8.001473\n",
      " 2720/5000: episode: 266, duration: 0.013s, episode steps: 9, steps per second: 677, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-2.811, 1.765], loss: 0.515802, mean_absolute_error: 4.148230, mean_q: 7.922648\n",
      " 2728/5000: episode: 267, duration: 0.012s, episode steps: 8, steps per second: 658, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.165 [-2.551, 1.551], loss: 0.490022, mean_absolute_error: 4.047371, mean_q: 7.684818\n",
      " 2739/5000: episode: 268, duration: 0.016s, episode steps: 11, steps per second: 677, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.138 [-2.797, 1.754], loss: 0.265856, mean_absolute_error: 4.247413, mean_q: 8.150453\n",
      " 2754/5000: episode: 269, duration: 0.022s, episode steps: 15, steps per second: 690, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.100 [-2.819, 1.740], loss: 0.357802, mean_absolute_error: 3.807007, mean_q: 7.224014\n",
      " 2765/5000: episode: 270, duration: 0.016s, episode steps: 11, steps per second: 673, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.116 [-2.744, 1.759], loss: 0.282415, mean_absolute_error: 4.011135, mean_q: 7.739418\n",
      " 2776/5000: episode: 271, duration: 0.016s, episode steps: 11, steps per second: 682, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.116 [-2.794, 1.756], loss: 0.393791, mean_absolute_error: 3.910283, mean_q: 7.490900\n",
      " 2786/5000: episode: 272, duration: 0.015s, episode steps: 10, steps per second: 657, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.133 [-2.997, 1.974], loss: 0.334331, mean_absolute_error: 4.028162, mean_q: 7.745774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2816/5000: episode: 273, duration: 0.043s, episode steps: 30, steps per second: 703, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.014 [-3.229, 2.288], loss: 0.272215, mean_absolute_error: 3.966243, mean_q: 7.624074\n",
      " 2834/5000: episode: 274, duration: 0.026s, episode steps: 18, steps per second: 681, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.084 [-2.550, 1.539], loss: 0.278259, mean_absolute_error: 3.882715, mean_q: 7.447523\n",
      " 2845/5000: episode: 275, duration: 0.016s, episode steps: 11, steps per second: 674, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.119 [-3.286, 2.174], loss: 0.292358, mean_absolute_error: 3.821169, mean_q: 7.320729\n",
      " 2855/5000: episode: 276, duration: 0.015s, episode steps: 10, steps per second: 667, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-3.059, 1.919], loss: 0.462090, mean_absolute_error: 3.990443, mean_q: 7.616151\n",
      " 2865/5000: episode: 277, duration: 0.015s, episode steps: 10, steps per second: 673, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.117 [-2.424, 1.525], loss: 0.295471, mean_absolute_error: 3.895441, mean_q: 7.460658\n",
      " 2875/5000: episode: 278, duration: 0.015s, episode steps: 10, steps per second: 667, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.128 [-3.000, 1.995], loss: 0.372662, mean_absolute_error: 4.032072, mean_q: 7.702673\n",
      " 2889/5000: episode: 279, duration: 0.020s, episode steps: 14, steps per second: 700, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.857 [0.000, 1.000], mean observation: -0.087 [-2.921, 1.909], loss: 0.330580, mean_absolute_error: 3.853842, mean_q: 7.378241\n",
      " 2924/5000: episode: 280, duration: 0.049s, episode steps: 35, steps per second: 717, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.686 [0.000, 1.000], mean observation: 0.033 [-3.319, 2.527], loss: 0.367465, mean_absolute_error: 3.882805, mean_q: 7.373427\n",
      " 2977/5000: episode: 281, duration: 0.074s, episode steps: 53, steps per second: 715, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.585 [0.000, 1.000], mean observation: -0.062 [-3.032, 1.817], loss: 0.308227, mean_absolute_error: 3.816208, mean_q: 7.242154\n",
      " 2987/5000: episode: 282, duration: 0.015s, episode steps: 10, steps per second: 671, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-3.058, 1.936], loss: 0.549592, mean_absolute_error: 3.864986, mean_q: 7.305089\n",
      " 3033/5000: episode: 283, duration: 0.062s, episode steps: 46, steps per second: 739, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: 0.057 [-3.229, 2.642], loss: 0.368643, mean_absolute_error: 3.896948, mean_q: 7.357105\n",
      " 3042/5000: episode: 284, duration: 0.014s, episode steps: 9, steps per second: 656, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.153 [-2.826, 1.748], loss: 0.334200, mean_absolute_error: 3.986606, mean_q: 7.515360\n",
      " 3052/5000: episode: 285, duration: 0.015s, episode steps: 10, steps per second: 675, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.130 [-2.633, 1.587], loss: 0.251200, mean_absolute_error: 3.926788, mean_q: 7.514398\n",
      " 3062/5000: episode: 286, duration: 0.015s, episode steps: 10, steps per second: 684, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.139 [-3.044, 1.917], loss: 0.356213, mean_absolute_error: 3.758488, mean_q: 7.151417\n",
      " 3072/5000: episode: 287, duration: 0.015s, episode steps: 10, steps per second: 676, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-3.016, 1.916], loss: 0.340751, mean_absolute_error: 3.774581, mean_q: 7.145544\n",
      " 3084/5000: episode: 288, duration: 0.017s, episode steps: 12, steps per second: 687, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.136 [-3.106, 1.950], loss: 0.366427, mean_absolute_error: 3.880735, mean_q: 7.307864\n",
      " 3103/5000: episode: 289, duration: 0.027s, episode steps: 19, steps per second: 711, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.789 [0.000, 1.000], mean observation: -0.025 [-3.126, 2.167], loss: 0.283640, mean_absolute_error: 3.720525, mean_q: 7.027968\n",
      " 3119/5000: episode: 290, duration: 0.027s, episode steps: 16, steps per second: 603, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.812 [0.000, 1.000], mean observation: -0.092 [-3.123, 1.944], loss: 0.252424, mean_absolute_error: 3.778202, mean_q: 7.186677\n",
      " 3129/5000: episode: 291, duration: 0.015s, episode steps: 10, steps per second: 659, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.125 [-3.064, 1.997], loss: 0.337297, mean_absolute_error: 3.818604, mean_q: 7.269296\n",
      " 3141/5000: episode: 292, duration: 0.018s, episode steps: 12, steps per second: 677, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.145 [-2.654, 1.551], loss: 0.343012, mean_absolute_error: 3.784706, mean_q: 7.120951\n",
      " 3151/5000: episode: 293, duration: 0.015s, episode steps: 10, steps per second: 662, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.114 [-2.528, 1.600], loss: 0.327026, mean_absolute_error: 3.878238, mean_q: 7.310327\n",
      " 3162/5000: episode: 294, duration: 0.016s, episode steps: 11, steps per second: 691, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.121 [-2.730, 1.740], loss: 0.441799, mean_absolute_error: 3.958854, mean_q: 7.442287\n",
      " 3170/5000: episode: 295, duration: 0.012s, episode steps: 8, steps per second: 669, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.148 [-2.175, 1.323], loss: 0.340675, mean_absolute_error: 3.827330, mean_q: 7.294366\n",
      " 3179/5000: episode: 296, duration: 0.014s, episode steps: 9, steps per second: 657, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-2.804, 1.749], loss: 0.340764, mean_absolute_error: 3.935810, mean_q: 7.507250\n",
      " 3190/5000: episode: 297, duration: 0.016s, episode steps: 11, steps per second: 680, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-3.267, 2.139], loss: 0.285927, mean_absolute_error: 3.880692, mean_q: 7.435567\n",
      " 3199/5000: episode: 298, duration: 0.013s, episode steps: 9, steps per second: 670, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.108 [-2.741, 1.800], loss: 0.407822, mean_absolute_error: 3.577802, mean_q: 6.741138\n",
      " 3209/5000: episode: 299, duration: 0.015s, episode steps: 10, steps per second: 682, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.129 [-3.036, 1.989], loss: 0.348033, mean_absolute_error: 3.727935, mean_q: 7.013406\n",
      " 3219/5000: episode: 300, duration: 0.015s, episode steps: 10, steps per second: 654, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-3.060, 1.993], loss: 0.313818, mean_absolute_error: 3.820201, mean_q: 7.201544\n",
      " 3229/5000: episode: 301, duration: 0.016s, episode steps: 10, steps per second: 641, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-3.110, 2.003], loss: 0.327763, mean_absolute_error: 3.716691, mean_q: 7.030452\n",
      " 3238/5000: episode: 302, duration: 0.014s, episode steps: 9, steps per second: 666, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.121 [-2.208, 1.357], loss: 0.378513, mean_absolute_error: 3.718838, mean_q: 6.985097\n",
      " 3248/5000: episode: 303, duration: 0.015s, episode steps: 10, steps per second: 663, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.178 [-2.628, 1.523], loss: 0.292013, mean_absolute_error: 3.719203, mean_q: 7.052148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3257/5000: episode: 304, duration: 0.016s, episode steps: 9, steps per second: 576, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.119 [-2.460, 1.605], loss: 0.344723, mean_absolute_error: 3.623901, mean_q: 6.852263\n",
      " 3267/5000: episode: 305, duration: 0.016s, episode steps: 10, steps per second: 644, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-3.044, 1.934], loss: 0.333039, mean_absolute_error: 3.818430, mean_q: 7.227118\n",
      " 3277/5000: episode: 306, duration: 0.015s, episode steps: 10, steps per second: 676, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.122 [-3.063, 1.990], loss: 0.232312, mean_absolute_error: 3.702419, mean_q: 7.051692\n",
      " 3288/5000: episode: 307, duration: 0.016s, episode steps: 11, steps per second: 676, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.105 [-2.768, 1.808], loss: 0.348602, mean_absolute_error: 3.503893, mean_q: 6.578257\n",
      " 3297/5000: episode: 308, duration: 0.014s, episode steps: 9, steps per second: 656, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-2.886, 1.746], loss: 0.408881, mean_absolute_error: 3.665299, mean_q: 6.838746\n",
      " 3307/5000: episode: 309, duration: 0.015s, episode steps: 10, steps per second: 665, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.108 [-2.982, 1.983], loss: 0.307026, mean_absolute_error: 3.711387, mean_q: 6.971253\n",
      " 3317/5000: episode: 310, duration: 0.015s, episode steps: 10, steps per second: 679, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.132 [-2.026, 1.170], loss: 0.222272, mean_absolute_error: 3.740602, mean_q: 7.114127\n",
      " 3328/5000: episode: 311, duration: 0.016s, episode steps: 11, steps per second: 687, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.124 [-2.748, 1.755], loss: 0.372864, mean_absolute_error: 3.661272, mean_q: 6.879544\n",
      " 3339/5000: episode: 312, duration: 0.017s, episode steps: 11, steps per second: 664, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.126 [-2.888, 1.808], loss: 0.425286, mean_absolute_error: 3.476026, mean_q: 6.505345\n",
      " 3348/5000: episode: 313, duration: 0.013s, episode steps: 9, steps per second: 679, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-2.788, 1.758], loss: 0.307966, mean_absolute_error: 3.575690, mean_q: 6.700844\n",
      " 3359/5000: episode: 314, duration: 0.016s, episode steps: 11, steps per second: 681, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.128 [-2.820, 1.796], loss: 0.361684, mean_absolute_error: 3.559987, mean_q: 6.669058\n",
      " 3369/5000: episode: 315, duration: 0.014s, episode steps: 10, steps per second: 690, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.129 [-3.066, 1.983], loss: 0.272831, mean_absolute_error: 3.775885, mean_q: 7.170458\n",
      " 3379/5000: episode: 316, duration: 0.015s, episode steps: 10, steps per second: 679, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.123 [-3.034, 1.996], loss: 0.294492, mean_absolute_error: 3.549135, mean_q: 6.710670\n",
      " 3387/5000: episode: 317, duration: 0.014s, episode steps: 8, steps per second: 575, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.173 [-2.579, 1.535], loss: 0.386309, mean_absolute_error: 3.525096, mean_q: 6.619658\n",
      " 3397/5000: episode: 318, duration: 0.016s, episode steps: 10, steps per second: 645, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-3.051, 1.958], loss: 0.424779, mean_absolute_error: 3.573031, mean_q: 6.715159\n",
      " 3407/5000: episode: 319, duration: 0.015s, episode steps: 10, steps per second: 667, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-3.087, 1.996], loss: 0.325667, mean_absolute_error: 3.510730, mean_q: 6.585706\n",
      " 3415/5000: episode: 320, duration: 0.012s, episode steps: 8, steps per second: 664, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.154 [-2.553, 1.574], loss: 0.247758, mean_absolute_error: 3.584681, mean_q: 6.783121\n",
      " 3426/5000: episode: 321, duration: 0.016s, episode steps: 11, steps per second: 682, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.123 [-2.863, 1.799], loss: 0.281454, mean_absolute_error: 3.599849, mean_q: 6.777385\n",
      " 3435/5000: episode: 322, duration: 0.013s, episode steps: 9, steps per second: 676, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.120 [-2.775, 1.799], loss: 0.292157, mean_absolute_error: 3.538151, mean_q: 6.632728\n",
      " 3465/5000: episode: 323, duration: 0.042s, episode steps: 30, steps per second: 716, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.095 [-0.395, 0.822], loss: 0.317623, mean_absolute_error: 3.512926, mean_q: 6.553817\n",
      " 3497/5000: episode: 324, duration: 0.045s, episode steps: 32, steps per second: 713, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.719 [0.000, 1.000], mean observation: 0.034 [-3.451, 2.684], loss: 0.352266, mean_absolute_error: 3.543968, mean_q: 6.603382\n",
      " 3512/5000: episode: 325, duration: 0.021s, episode steps: 15, steps per second: 704, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.074 [-2.153, 1.336], loss: 0.437325, mean_absolute_error: 3.563229, mean_q: 6.566897\n",
      " 3534/5000: episode: 326, duration: 0.033s, episode steps: 22, steps per second: 673, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.064 [-3.137, 2.005], loss: 0.321588, mean_absolute_error: 3.447874, mean_q: 6.375860\n",
      " 3546/5000: episode: 327, duration: 0.017s, episode steps: 12, steps per second: 692, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.092 [-3.012, 1.978], loss: 0.380616, mean_absolute_error: 3.698495, mean_q: 6.855104\n",
      " 3557/5000: episode: 328, duration: 0.016s, episode steps: 11, steps per second: 668, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.115 [-2.326, 1.373], loss: 0.414520, mean_absolute_error: 3.518633, mean_q: 6.493277\n",
      " 3567/5000: episode: 329, duration: 0.015s, episode steps: 10, steps per second: 661, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.136 [-2.492, 1.575], loss: 0.471770, mean_absolute_error: 3.545251, mean_q: 6.508183\n",
      " 3577/5000: episode: 330, duration: 0.015s, episode steps: 10, steps per second: 660, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-3.053, 1.908], loss: 0.272398, mean_absolute_error: 3.555169, mean_q: 6.662259\n",
      " 3587/5000: episode: 331, duration: 0.015s, episode steps: 10, steps per second: 659, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-3.028, 1.913], loss: 0.274750, mean_absolute_error: 3.556498, mean_q: 6.648604\n",
      " 3610/5000: episode: 332, duration: 0.033s, episode steps: 23, steps per second: 694, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.739 [0.000, 1.000], mean observation: -0.059 [-3.306, 2.120], loss: 0.357088, mean_absolute_error: 3.485536, mean_q: 6.469473\n",
      " 3619/5000: episode: 333, duration: 0.014s, episode steps: 9, steps per second: 641, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-2.769, 1.752], loss: 0.303485, mean_absolute_error: 3.688327, mean_q: 6.925245\n",
      " 3627/5000: episode: 334, duration: 0.012s, episode steps: 8, steps per second: 646, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-2.543, 1.534], loss: 0.376940, mean_absolute_error: 3.270307, mean_q: 6.017070\n",
      " 3636/5000: episode: 335, duration: 0.014s, episode steps: 9, steps per second: 633, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.153 [-2.818, 1.769], loss: 0.243105, mean_absolute_error: 3.314561, mean_q: 6.201993\n",
      " 3646/5000: episode: 336, duration: 0.016s, episode steps: 10, steps per second: 637, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-3.033, 1.905], loss: 0.313361, mean_absolute_error: 3.358577, mean_q: 6.270329\n",
      " 3657/5000: episode: 337, duration: 0.018s, episode steps: 11, steps per second: 598, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.133 [-3.289, 2.166], loss: 0.405085, mean_absolute_error: 3.264890, mean_q: 6.055043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3668/5000: episode: 338, duration: 0.019s, episode steps: 11, steps per second: 588, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.120 [-3.289, 2.144], loss: 0.462506, mean_absolute_error: 3.484932, mean_q: 6.378937\n",
      " 3677/5000: episode: 339, duration: 0.014s, episode steps: 9, steps per second: 629, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-2.804, 1.763], loss: 0.332764, mean_absolute_error: 3.612020, mean_q: 6.698168\n",
      " 3689/5000: episode: 340, duration: 0.018s, episode steps: 12, steps per second: 681, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.094 [-3.032, 1.992], loss: 0.483753, mean_absolute_error: 3.585841, mean_q: 6.604118\n",
      " 3699/5000: episode: 341, duration: 0.016s, episode steps: 10, steps per second: 640, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-3.006, 1.963], loss: 0.384803, mean_absolute_error: 3.327751, mean_q: 6.113815\n",
      " 3709/5000: episode: 342, duration: 0.015s, episode steps: 10, steps per second: 669, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-2.973, 1.937], loss: 0.341073, mean_absolute_error: 3.157031, mean_q: 5.841979\n",
      " 3718/5000: episode: 343, duration: 0.014s, episode steps: 9, steps per second: 641, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-2.840, 1.735], loss: 0.480671, mean_absolute_error: 3.252206, mean_q: 5.964787\n",
      " 3727/5000: episode: 344, duration: 0.014s, episode steps: 9, steps per second: 662, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-2.779, 1.723], loss: 0.325783, mean_absolute_error: 3.161305, mean_q: 5.804018\n",
      " 3737/5000: episode: 345, duration: 0.015s, episode steps: 10, steps per second: 662, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.118 [-1.962, 1.191], loss: 0.333965, mean_absolute_error: 3.593104, mean_q: 6.638957\n",
      " 3746/5000: episode: 346, duration: 0.014s, episode steps: 9, steps per second: 663, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-2.846, 1.803], loss: 0.295651, mean_absolute_error: 3.396800, mean_q: 6.253703\n",
      " 3756/5000: episode: 347, duration: 0.015s, episode steps: 10, steps per second: 668, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.120 [-3.032, 1.972], loss: 0.375076, mean_absolute_error: 3.416810, mean_q: 6.280910\n",
      " 3767/5000: episode: 348, duration: 0.016s, episode steps: 11, steps per second: 672, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.131 [-2.845, 1.810], loss: 0.259275, mean_absolute_error: 3.408575, mean_q: 6.383730\n",
      " 3776/5000: episode: 349, duration: 0.014s, episode steps: 9, steps per second: 663, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.129 [-2.808, 1.796], loss: 0.302973, mean_absolute_error: 3.291268, mean_q: 6.119061\n",
      " 3785/5000: episode: 350, duration: 0.014s, episode steps: 9, steps per second: 666, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.166 [-2.513, 1.529], loss: 0.289751, mean_absolute_error: 3.158565, mean_q: 5.839906\n",
      " 3795/5000: episode: 351, duration: 0.017s, episode steps: 10, steps per second: 584, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.124 [-2.475, 1.555], loss: 0.482676, mean_absolute_error: 3.402516, mean_q: 6.263118\n",
      " 3843/5000: episode: 352, duration: 0.067s, episode steps: 48, steps per second: 720, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.646 [0.000, 1.000], mean observation: 0.014 [-3.563, 2.701], loss: 0.356030, mean_absolute_error: 3.355660, mean_q: 6.155575\n",
      " 3861/5000: episode: 353, duration: 0.026s, episode steps: 18, steps per second: 704, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.049 [-3.386, 2.309], loss: 0.281616, mean_absolute_error: 3.331863, mean_q: 6.159851\n",
      " 3871/5000: episode: 354, duration: 0.015s, episode steps: 10, steps per second: 673, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.134 [-3.030, 1.998], loss: 0.390878, mean_absolute_error: 3.568425, mean_q: 6.554255\n",
      " 3879/5000: episode: 355, duration: 0.012s, episode steps: 8, steps per second: 675, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.153 [-2.579, 1.546], loss: 0.507435, mean_absolute_error: 3.321462, mean_q: 6.044683\n",
      " 3891/5000: episode: 356, duration: 0.017s, episode steps: 12, steps per second: 687, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.131 [-2.642, 1.589], loss: 0.344267, mean_absolute_error: 3.295197, mean_q: 6.092044\n",
      " 3901/5000: episode: 357, duration: 0.015s, episode steps: 10, steps per second: 670, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.124 [-2.548, 1.611], loss: 0.351724, mean_absolute_error: 3.278930, mean_q: 6.021812\n",
      " 3910/5000: episode: 358, duration: 0.014s, episode steps: 9, steps per second: 653, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.155 [-2.749, 1.720], loss: 0.340460, mean_absolute_error: 3.365838, mean_q: 6.211001\n",
      " 3919/5000: episode: 359, duration: 0.014s, episode steps: 9, steps per second: 663, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-2.808, 1.788], loss: 0.331984, mean_absolute_error: 3.338386, mean_q: 6.155873\n",
      " 3927/5000: episode: 360, duration: 0.014s, episode steps: 8, steps per second: 591, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-2.575, 1.613], loss: 0.274134, mean_absolute_error: 3.249107, mean_q: 6.008824\n",
      " 3936/5000: episode: 361, duration: 0.014s, episode steps: 9, steps per second: 636, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-2.810, 1.735], loss: 0.238247, mean_absolute_error: 3.211957, mean_q: 5.997449\n",
      " 3945/5000: episode: 362, duration: 0.014s, episode steps: 9, steps per second: 646, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-2.751, 1.722], loss: 0.280339, mean_absolute_error: 2.951245, mean_q: 5.477928\n",
      " 3959/5000: episode: 363, duration: 0.020s, episode steps: 14, steps per second: 688, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.857 [0.000, 1.000], mean observation: -0.093 [-3.032, 1.929], loss: 0.347543, mean_absolute_error: 3.270830, mean_q: 6.027153\n",
      " 3969/5000: episode: 364, duration: 0.015s, episode steps: 10, steps per second: 657, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-3.088, 1.994], loss: 0.415874, mean_absolute_error: 3.308510, mean_q: 6.076086\n",
      " 3978/5000: episode: 365, duration: 0.014s, episode steps: 9, steps per second: 659, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-2.843, 1.807], loss: 0.381127, mean_absolute_error: 3.137088, mean_q: 5.717515\n",
      " 3987/5000: episode: 366, duration: 0.014s, episode steps: 9, steps per second: 660, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-2.758, 1.771], loss: 0.365875, mean_absolute_error: 3.198253, mean_q: 5.880915\n",
      " 3999/5000: episode: 367, duration: 0.018s, episode steps: 12, steps per second: 684, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.113 [-3.029, 1.998], loss: 0.384461, mean_absolute_error: 3.245689, mean_q: 5.982741\n",
      " 4008/5000: episode: 368, duration: 0.013s, episode steps: 9, steps per second: 672, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-2.817, 1.761], loss: 0.271864, mean_absolute_error: 3.141658, mean_q: 5.853553\n",
      " 4018/5000: episode: 369, duration: 0.015s, episode steps: 10, steps per second: 670, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-3.077, 1.991], loss: 0.421173, mean_absolute_error: 3.320992, mean_q: 6.139084\n",
      " 4028/5000: episode: 370, duration: 0.015s, episode steps: 10, steps per second: 684, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-3.067, 1.920], loss: 0.489979, mean_absolute_error: 3.204697, mean_q: 5.803002\n",
      " 4038/5000: episode: 371, duration: 0.015s, episode steps: 10, steps per second: 672, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.143 [-2.546, 1.579], loss: 0.310037, mean_absolute_error: 3.153147, mean_q: 5.795630\n",
      " 4047/5000: episode: 372, duration: 0.014s, episode steps: 9, steps per second: 666, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-2.784, 1.748], loss: 0.327954, mean_absolute_error: 3.360393, mean_q: 6.192057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4059/5000: episode: 373, duration: 0.020s, episode steps: 12, steps per second: 598, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.128 [-2.734, 1.780], loss: 0.540008, mean_absolute_error: 3.303685, mean_q: 5.998034\n",
      " 4068/5000: episode: 374, duration: 0.014s, episode steps: 9, steps per second: 634, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.144 [-2.472, 1.580], loss: 0.389310, mean_absolute_error: 3.284341, mean_q: 6.018667\n",
      " 4080/5000: episode: 375, duration: 0.018s, episode steps: 12, steps per second: 681, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.111 [-2.703, 1.790], loss: 0.391832, mean_absolute_error: 3.085925, mean_q: 5.624223\n",
      " 4090/5000: episode: 376, duration: 0.015s, episode steps: 10, steps per second: 656, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.135 [-2.359, 1.515], loss: 0.477735, mean_absolute_error: 3.226402, mean_q: 5.850815\n",
      " 4101/5000: episode: 377, duration: 0.016s, episode steps: 11, steps per second: 673, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.125 [-2.371, 1.522], loss: 0.430333, mean_absolute_error: 3.161460, mean_q: 5.797776\n",
      " 4115/5000: episode: 378, duration: 0.020s, episode steps: 14, steps per second: 690, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.065 [-2.292, 1.549], loss: 0.285916, mean_absolute_error: 3.235544, mean_q: 6.006678\n",
      " 4125/5000: episode: 379, duration: 0.015s, episode steps: 10, steps per second: 673, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.140 [-2.202, 1.375], loss: 0.319365, mean_absolute_error: 3.067624, mean_q: 5.702143\n",
      " 4135/5000: episode: 380, duration: 0.015s, episode steps: 10, steps per second: 658, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.109 [-2.345, 1.555], loss: 0.311697, mean_absolute_error: 2.959226, mean_q: 5.424888\n",
      " 4146/5000: episode: 381, duration: 0.017s, episode steps: 11, steps per second: 659, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.129 [-2.379, 1.516], loss: 0.312826, mean_absolute_error: 3.123289, mean_q: 5.773460\n",
      " 4155/5000: episode: 382, duration: 0.014s, episode steps: 9, steps per second: 660, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.161 [-2.204, 1.354], loss: 0.398739, mean_absolute_error: 3.194610, mean_q: 5.866148\n",
      " 4165/5000: episode: 383, duration: 0.015s, episode steps: 10, steps per second: 674, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.144 [-2.285, 1.366], loss: 0.337068, mean_absolute_error: 3.158629, mean_q: 5.794374\n",
      " 4177/5000: episode: 384, duration: 0.018s, episode steps: 12, steps per second: 683, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.130 [-2.140, 1.320], loss: 0.384768, mean_absolute_error: 3.292279, mean_q: 6.010380\n",
      " 4186/5000: episode: 385, duration: 0.013s, episode steps: 9, steps per second: 670, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.150 [-2.262, 1.373], loss: 0.363969, mean_absolute_error: 3.061466, mean_q: 5.563815\n",
      " 4196/5000: episode: 386, duration: 0.018s, episode steps: 10, steps per second: 545, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.135 [-2.411, 1.603], loss: 0.309131, mean_absolute_error: 3.255365, mean_q: 6.034600\n",
      " 4206/5000: episode: 387, duration: 0.015s, episode steps: 10, steps per second: 665, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.128 [-2.342, 1.557], loss: 0.373514, mean_absolute_error: 2.973130, mean_q: 5.421995\n",
      " 4217/5000: episode: 388, duration: 0.016s, episode steps: 11, steps per second: 672, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.119 [-2.082, 1.318], loss: 0.368288, mean_absolute_error: 3.256446, mean_q: 6.001184\n",
      " 4226/5000: episode: 389, duration: 0.014s, episode steps: 9, steps per second: 630, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.150 [-1.931, 1.152], loss: 0.377677, mean_absolute_error: 3.336272, mean_q: 6.143586\n",
      " 4236/5000: episode: 390, duration: 0.015s, episode steps: 10, steps per second: 658, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.145 [-1.970, 1.143], loss: 0.376212, mean_absolute_error: 3.220024, mean_q: 5.860414\n",
      " 4247/5000: episode: 391, duration: 0.016s, episode steps: 11, steps per second: 674, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.113 [-1.563, 1.007], loss: 0.421894, mean_absolute_error: 3.249187, mean_q: 5.911901\n",
      " 4262/5000: episode: 392, duration: 0.023s, episode steps: 15, steps per second: 660, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.081 [-1.553, 1.012], loss: 0.326671, mean_absolute_error: 3.170010, mean_q: 5.786388\n",
      " 4275/5000: episode: 393, duration: 0.020s, episode steps: 13, steps per second: 656, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.092 [-1.505, 1.001], loss: 0.297154, mean_absolute_error: 3.073796, mean_q: 5.635326\n",
      " 4286/5000: episode: 394, duration: 0.018s, episode steps: 11, steps per second: 607, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.113 [-1.769, 1.136], loss: 0.386910, mean_absolute_error: 3.138524, mean_q: 5.697194\n",
      " 4297/5000: episode: 395, duration: 0.017s, episode steps: 11, steps per second: 631, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.115 [-1.275, 0.771], loss: 0.418521, mean_absolute_error: 3.102772, mean_q: 5.610214\n",
      " 4312/5000: episode: 396, duration: 0.023s, episode steps: 15, steps per second: 663, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.076 [-1.553, 0.964], loss: 0.476026, mean_absolute_error: 3.256351, mean_q: 5.910666\n",
      " 4323/5000: episode: 397, duration: 0.017s, episode steps: 11, steps per second: 632, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.084 [-1.229, 0.819], loss: 0.445619, mean_absolute_error: 3.249877, mean_q: 5.895177\n",
      " 4336/5000: episode: 398, duration: 0.021s, episode steps: 13, steps per second: 630, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.090 [-1.294, 0.800], loss: 0.352572, mean_absolute_error: 3.108249, mean_q: 5.679108\n",
      " 4348/5000: episode: 399, duration: 0.021s, episode steps: 12, steps per second: 584, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.096 [-1.814, 1.138], loss: 0.467553, mean_absolute_error: 3.189262, mean_q: 5.759109\n",
      " 4359/5000: episode: 400, duration: 0.017s, episode steps: 11, steps per second: 645, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.114 [-1.777, 1.189], loss: 0.386524, mean_absolute_error: 3.217213, mean_q: 5.902020\n",
      " 4371/5000: episode: 401, duration: 0.018s, episode steps: 12, steps per second: 667, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.120 [-1.996, 1.319], loss: 0.406122, mean_absolute_error: 3.137128, mean_q: 5.755392\n",
      " 4381/5000: episode: 402, duration: 0.016s, episode steps: 10, steps per second: 639, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.137 [-2.084, 1.334], loss: 0.389719, mean_absolute_error: 3.036460, mean_q: 5.562087\n",
      " 4393/5000: episode: 403, duration: 0.022s, episode steps: 12, steps per second: 550, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.108 [-1.768, 1.184], loss: 0.406050, mean_absolute_error: 3.084795, mean_q: 5.641515\n",
      " 4403/5000: episode: 404, duration: 0.015s, episode steps: 10, steps per second: 648, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.123 [-1.840, 1.149], loss: 0.450502, mean_absolute_error: 3.042098, mean_q: 5.547828\n",
      " 4415/5000: episode: 405, duration: 0.019s, episode steps: 12, steps per second: 647, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.107 [-1.727, 0.999], loss: 0.352304, mean_absolute_error: 3.165798, mean_q: 5.819954\n",
      " 4425/5000: episode: 406, duration: 0.016s, episode steps: 10, steps per second: 642, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.126 [-1.613, 0.982], loss: 0.414573, mean_absolute_error: 3.289084, mean_q: 6.031371\n",
      " 4437/5000: episode: 407, duration: 0.018s, episode steps: 12, steps per second: 657, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.117 [-1.398, 0.756], loss: 0.506962, mean_absolute_error: 3.253571, mean_q: 5.923411\n",
      " 4448/5000: episode: 408, duration: 0.017s, episode steps: 11, steps per second: 664, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.112 [-1.527, 0.966], loss: 0.459529, mean_absolute_error: 3.078399, mean_q: 5.600585\n",
      " 4459/5000: episode: 409, duration: 0.017s, episode steps: 11, steps per second: 656, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.112 [-1.760, 1.145], loss: 0.452408, mean_absolute_error: 3.028486, mean_q: 5.506326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4469/5000: episode: 410, duration: 0.020s, episode steps: 10, steps per second: 505, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.144 [-1.859, 1.147], loss: 0.377574, mean_absolute_error: 3.027922, mean_q: 5.591921\n",
      " 4479/5000: episode: 411, duration: 0.018s, episode steps: 10, steps per second: 568, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.125 [-1.830, 1.216], loss: 0.453219, mean_absolute_error: 3.061948, mean_q: 5.620577\n",
      " 4490/5000: episode: 412, duration: 0.019s, episode steps: 11, steps per second: 568, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.098 [-1.731, 1.196], loss: 0.396594, mean_absolute_error: 3.049589, mean_q: 5.580395\n",
      " 4503/5000: episode: 413, duration: 0.025s, episode steps: 13, steps per second: 526, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.083 [-1.810, 1.199], loss: 0.462538, mean_absolute_error: 3.084310, mean_q: 5.652002\n",
      " 4514/5000: episode: 414, duration: 0.021s, episode steps: 11, steps per second: 515, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.089 [-2.055, 1.410], loss: 0.414340, mean_absolute_error: 3.110606, mean_q: 5.703725\n",
      " 4524/5000: episode: 415, duration: 0.018s, episode steps: 10, steps per second: 570, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.121 [-2.072, 1.352], loss: 0.444622, mean_absolute_error: 3.137716, mean_q: 5.777337\n",
      " 4534/5000: episode: 416, duration: 0.019s, episode steps: 10, steps per second: 525, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.121 [-1.856, 1.196], loss: 0.403251, mean_absolute_error: 3.109430, mean_q: 5.749569\n",
      " 4545/5000: episode: 417, duration: 0.022s, episode steps: 11, steps per second: 508, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.090 [-1.792, 1.191], loss: 0.463902, mean_absolute_error: 3.156537, mean_q: 5.811322\n",
      " 4557/5000: episode: 418, duration: 0.022s, episode steps: 12, steps per second: 545, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.086 [-2.001, 1.386], loss: 0.475395, mean_absolute_error: 3.208679, mean_q: 5.905876\n",
      " 4569/5000: episode: 419, duration: 0.021s, episode steps: 12, steps per second: 571, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.083 [-2.006, 1.411], loss: 0.531534, mean_absolute_error: 3.160697, mean_q: 5.789094\n",
      " 4580/5000: episode: 420, duration: 0.025s, episode steps: 11, steps per second: 440, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.130 [-1.806, 1.141], loss: 0.486739, mean_absolute_error: 3.190667, mean_q: 5.838260\n",
      " 4594/5000: episode: 421, duration: 0.026s, episode steps: 14, steps per second: 536, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.057 [-1.723, 1.201], loss: 0.578322, mean_absolute_error: 3.147990, mean_q: 5.689452\n",
      " 4606/5000: episode: 422, duration: 0.027s, episode steps: 12, steps per second: 448, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.077 [-1.743, 1.213], loss: 0.468224, mean_absolute_error: 3.110851, mean_q: 5.688955\n",
      " 4615/5000: episode: 423, duration: 0.018s, episode steps: 9, steps per second: 512, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.160 [-2.198, 1.367], loss: 0.476729, mean_absolute_error: 3.193195, mean_q: 5.885173\n",
      " 4625/5000: episode: 424, duration: 0.019s, episode steps: 10, steps per second: 523, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.120 [-2.103, 1.364], loss: 0.439153, mean_absolute_error: 3.106646, mean_q: 5.744946\n",
      " 4637/5000: episode: 425, duration: 0.022s, episode steps: 12, steps per second: 538, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.104 [-1.916, 1.174], loss: 0.347320, mean_absolute_error: 3.072614, mean_q: 5.728945\n",
      " 4646/5000: episode: 426, duration: 0.017s, episode steps: 9, steps per second: 520, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.135 [-1.850, 1.158], loss: 0.597868, mean_absolute_error: 3.252764, mean_q: 5.949175\n",
      " 4656/5000: episode: 427, duration: 0.022s, episode steps: 10, steps per second: 453, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.108 [-1.806, 1.199], loss: 0.410667, mean_absolute_error: 3.124784, mean_q: 5.794879\n",
      " 4669/5000: episode: 428, duration: 0.025s, episode steps: 13, steps per second: 523, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.081 [-1.709, 1.174], loss: 0.363300, mean_absolute_error: 3.125101, mean_q: 5.812372\n",
      " 4682/5000: episode: 429, duration: 0.025s, episode steps: 13, steps per second: 511, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.078 [-1.707, 1.186], loss: 0.386549, mean_absolute_error: 3.087173, mean_q: 5.759603\n",
      " 4691/5000: episode: 430, duration: 0.018s, episode steps: 9, steps per second: 497, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.144 [-1.880, 1.171], loss: 0.433646, mean_absolute_error: 3.015546, mean_q: 5.619301\n",
      " 4702/5000: episode: 431, duration: 0.021s, episode steps: 11, steps per second: 533, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.121 [-2.041, 1.356], loss: 0.549417, mean_absolute_error: 3.116482, mean_q: 5.716175\n",
      " 4714/5000: episode: 432, duration: 0.023s, episode steps: 12, steps per second: 526, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.100 [-2.094, 1.407], loss: 0.405349, mean_absolute_error: 3.127956, mean_q: 5.793217\n",
      " 4725/5000: episode: 433, duration: 0.020s, episode steps: 11, steps per second: 562, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.102 [-1.762, 1.138], loss: 0.321147, mean_absolute_error: 2.938468, mean_q: 5.496564\n",
      " 4734/5000: episode: 434, duration: 0.016s, episode steps: 9, steps per second: 560, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.137 [-2.102, 1.334], loss: 0.503666, mean_absolute_error: 3.126184, mean_q: 5.773229\n",
      " 4747/5000: episode: 435, duration: 0.021s, episode steps: 13, steps per second: 607, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.077 [-1.751, 1.209], loss: 0.458922, mean_absolute_error: 3.110942, mean_q: 5.783796\n",
      " 4758/5000: episode: 436, duration: 0.019s, episode steps: 11, steps per second: 582, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.125 [-1.577, 0.984], loss: 0.408176, mean_absolute_error: 2.957748, mean_q: 5.464065\n",
      " 4784/5000: episode: 437, duration: 0.044s, episode steps: 26, steps per second: 593, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.099 [-1.262, 0.787], loss: 0.471076, mean_absolute_error: 3.139131, mean_q: 5.813985\n",
      " 4795/5000: episode: 438, duration: 0.020s, episode steps: 11, steps per second: 541, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.121 [-2.079, 1.399], loss: 0.584140, mean_absolute_error: 3.105179, mean_q: 5.641561\n",
      " 4825/5000: episode: 439, duration: 0.051s, episode steps: 30, steps per second: 591, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.152 [-1.229, 0.816], loss: 0.513346, mean_absolute_error: 3.168457, mean_q: 5.848011\n",
      " 4837/5000: episode: 440, duration: 0.020s, episode steps: 12, steps per second: 613, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.113 [-1.516, 0.956], loss: 0.414218, mean_absolute_error: 3.136668, mean_q: 5.827835\n",
      " 4865/5000: episode: 441, duration: 0.043s, episode steps: 28, steps per second: 656, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.114 [-1.499, 1.014], loss: 0.551738, mean_absolute_error: 3.187893, mean_q: 5.876565\n",
      " 4908/5000: episode: 442, duration: 0.063s, episode steps: 43, steps per second: 687, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.160 [-1.142, 0.744], loss: 0.471848, mean_absolute_error: 3.138965, mean_q: 5.749562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4950/5000: episode: 443, duration: 0.067s, episode steps: 42, steps per second: 626, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.155 [-0.728, 0.388], loss: 0.568587, mean_absolute_error: 3.276893, mean_q: 5.975963\n",
      "done, took 8.300 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1894502c940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=5000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 78.000, steps: 78\n",
      "Episode 2: reward: 64.000, steps: 64\n",
      "Episode 3: reward: 41.000, steps: 41\n",
      "Episode 4: reward: 64.000, steps: 64\n",
      "Episode 5: reward: 50.000, steps: 50\n",
      "Episode 6: reward: 44.000, steps: 44\n",
      "Episode 7: reward: 39.000, steps: 39\n",
      "Episode 8: reward: 72.000, steps: 72\n",
      "Episode 9: reward: 48.000, steps: 48\n",
      "Episode 10: reward: 36.000, steps: 36\n"
     ]
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Car Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Defining the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'MountainCar-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions available in the Cartpole problem\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Defining the ANN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                192       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 51        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 1,555\n",
      "Trainable params: 1,555\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: Training the Model using the Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 16s 2ms/step - reward: -1.0000\n",
      "done, took 16.024 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1894fa7aba8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=400,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=10000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -200.000, steps: 200\n",
      "Episode 2: reward: -200.000, steps: 200\n",
      "Episode 3: reward: -200.000, steps: 200\n",
      "Episode 4: reward: -200.000, steps: 200\n",
      "Episode 5: reward: -200.000, steps: 200\n"
     ]
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5: Random Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "action = 0\n",
    "for i in range(1000):\n",
    "    env.render()\n",
    "    if i < 400:\n",
    "        env.step(0) # take a random action\n",
    "    else:\n",
    "        env.step(2) # take a random action\n",
    "        # env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
